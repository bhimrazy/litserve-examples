{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65989bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70863205\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "\n",
    "# Initialize the model\n",
    "model_id = \"jinaai/jina-embeddings-v3\"\n",
    "model = AutoModel.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"Follow the white rabbit.\",  # English\n",
    "    \"Sigue al conejo blanco.\",  # Spanish\n",
    "    \"Suis le lapin blanc.\",  # French\n",
    "    \"跟着白兔走。\",  # Chinese\n",
    "    \"اتبع الأرنب الأبيض.\",  # Arabic\n",
    "    \"Folge dem weißen Kaninchen.\",  # German\n",
    "]\n",
    "\n",
    "# When calling the `encode` function, you can choose a `task` based on the use case:\n",
    "# 'retrieval.query', 'retrieval.passage', 'separation', 'classification', 'text-matching'\n",
    "# Alternatively, you can choose not to pass a `task`, and no specific LoRA adapter will be used.\n",
    "embeddings = model.encode(texts, task=\"text-matching\")\n",
    "\n",
    "# Compute similarities\n",
    "print(embeddings[0] @ embeddings[1].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61fbdbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72f9c42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Follow',\n",
       " 'the',\n",
       " 'white',\n",
       " 'rabbit.',\n",
       " 'Sigue',\n",
       " 'al',\n",
       " 'conejo',\n",
       " 'blanco.',\n",
       " 'Suis',\n",
       " 'le',\n",
       " 'lapin',\n",
       " 'blanc.',\n",
       " '跟着白兔走。',\n",
       " 'اتبع',\n",
       " 'الأرنب',\n",
       " 'الأبيض.',\n",
       " 'Folge',\n",
       " 'dem',\n",
       " 'weißen',\n",
       " 'Kaninchen.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = \"\\n\".join(texts)\n",
    "document.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c8960f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs length: [24, 23, 20, 6, 19, 27]\n",
      "Cumulative inputs length: [  0  24  47  67  73  92 119]\n",
      "Span annotations: [(0, 24), (24, 47), (47, 67), (67, 73), (73, 92), (92, 119)]\n",
      "Document: Follow the white rabbit.<SEP>Sigue al conejo blanco.<SEP>Suis le lapin blanc.<SEP>跟着白兔走。<SEP>اتبع الأرنب الأبيض.<SEP>Folge dem weißen Kaninchen.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "SEPARATOR_TOKEN = \"<SEP>\"\n",
    "SEPARATOR_TOKEN_ID = tokenizer.convert_tokens_to_ids(SEPARATOR_TOKEN)\n",
    "# Combine all texts into a single document\n",
    "document = SEPARATOR_TOKEN.join(texts)\n",
    "\n",
    "# Calculate the length of each text\n",
    "inputs_length = [len(text) for text in texts]\n",
    "print(\"Inputs length:\", inputs_length)\n",
    "\n",
    "# Compute cumulative lengths\n",
    "cum_inputs_length = np.cumsum([0] + inputs_length)\n",
    "print(\"Cumulative inputs length:\", cum_inputs_length)\n",
    "\n",
    "# Generate span annotations as tuples of start and end indices\n",
    "span_annotations = [(start, end) for start, end in zip(cum_inputs_length[:-1], cum_inputs_length[1:])]\n",
    "print(\"Span annotations:\", span_annotations)\n",
    "\n",
    "print(\"Document:\", document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6df61137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0,  77168,     70,  35011, 152131,     18, 100052,    294,  21290,\n",
       "           2740,   8859,   6261,    144,    158,  18039, 101455, 100052,    294,\n",
       "          21290,   2740,  14168,    164,     95,     21,   5128,  38972, 100052,\n",
       "            294,  21290,   2740, 113998,   3515, 130818,   3469,     30,  16093,\n",
       "            294,  21290,   2740,    396,  46776,  86218,  44573, 151721, 100052,\n",
       "            294,  21290,   2740,  27591,  54090,    745,  23739,     33,   2734,\n",
       "             73,   4834,      5,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'offset_mapping': tensor([[[  0,   0],\n",
       "         [  0,   6],\n",
       "         [  7,  10],\n",
       "         [ 11,  16],\n",
       "         [ 17,  22],\n",
       "         [ 22,  23],\n",
       "         [ 23,  25],\n",
       "         [ 25,  26],\n",
       "         [ 26,  28],\n",
       "         [ 28,  29],\n",
       "         [ 29,  31],\n",
       "         [ 31,  34],\n",
       "         [ 35,  37],\n",
       "         [ 38,  41],\n",
       "         [ 41,  44],\n",
       "         [ 45,  51],\n",
       "         [ 51,  53],\n",
       "         [ 53,  54],\n",
       "         [ 54,  56],\n",
       "         [ 56,  57],\n",
       "         [ 57,  59],\n",
       "         [ 59,  61],\n",
       "         [ 62,  64],\n",
       "         [ 65,  67],\n",
       "         [ 67,  70],\n",
       "         [ 71,  76],\n",
       "         [ 76,  78],\n",
       "         [ 78,  79],\n",
       "         [ 79,  81],\n",
       "         [ 81,  82],\n",
       "         [ 82,  84],\n",
       "         [ 84,  85],\n",
       "         [ 85,  86],\n",
       "         [ 86,  87],\n",
       "         [ 87,  88],\n",
       "         [ 88,  89],\n",
       "         [ 89,  90],\n",
       "         [ 90,  92],\n",
       "         [ 92,  93],\n",
       "         [ 93,  95],\n",
       "         [ 95,  97],\n",
       "         [ 98, 102],\n",
       "         [102, 104],\n",
       "         [105, 111],\n",
       "         [111, 113],\n",
       "         [113, 114],\n",
       "         [114, 116],\n",
       "         [116, 117],\n",
       "         [117, 119],\n",
       "         [119, 122],\n",
       "         [123, 126],\n",
       "         [127, 131],\n",
       "         [131, 133],\n",
       "         [134, 137],\n",
       "         [137, 139],\n",
       "         [139, 143],\n",
       "         [143, 144],\n",
       "         [  0,   0]]])}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    document,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b457c42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 10]), torch.Size([6, 10]), torch.Size([6, 10, 2]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"].shape, inputs[\"attention_mask\"].shape, inputs[\"offset_mapping\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "63a6e281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \n",
      "Text: Follow\n",
      "Text: the\n",
      "Text: white\n",
      "Text: rabbi\n",
      "Text: t\n",
      "Text: .<\n",
      "Text: S\n",
      "Text: EP\n",
      "Text: >\n",
      "Text: Si\n",
      "Text: gue\n",
      "Text: al\n",
      "Text: con\n",
      "Text: ejo\n",
      "Text: blanco\n",
      "Text: .<\n",
      "Text: S\n",
      "Text: EP\n",
      "Text: >\n",
      "Text: Su\n",
      "Text: is\n",
      "Text: le\n",
      "Text: la\n",
      "Text: pin\n",
      "Text: blanc\n",
      "Text: .<\n",
      "Text: S\n",
      "Text: EP\n",
      "Text: >\n",
      "Text: 跟着\n",
      "Text: 白\n",
      "Text: 兔\n",
      "Text: 走\n",
      "Text: 。\n",
      "Text: <\n",
      "Text: S\n",
      "Text: EP\n",
      "Text: >\n",
      "Text: ات\n",
      "Text: بع\n",
      "Text: الأر\n",
      "Text: نب\n",
      "Text: الأبيض\n",
      "Text: .<\n",
      "Text: S\n",
      "Text: EP\n",
      "Text: >\n",
      "Text: Fo\n",
      "Text: lge\n",
      "Text: dem\n",
      "Text: weiß\n",
      "Text: en\n",
      "Text: Kan\n",
      "Text: in\n",
      "Text: chen\n",
      "Text: .\n",
      "Text: \n"
     ]
    }
   ],
   "source": [
    "offset_mapping = inputs[\"offset_mapping\"][0]\n",
    "\n",
    "for start, end in offset_mapping:\n",
    "    # Get the start and end token indices for each text\n",
    "    print(f\"Text: {document[start:end]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e3084d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 24), (24, 47), (47, 67), (67, 73), (73, 92), (92, 119)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaa0bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 6), (7, 13), (14, 20), (21, 25), (26, 31), (32, 40)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_positions = []\n",
    "cum_inputs_length, inputs[\"offset_mapping\"]\n",
    "\n",
    "\n",
    "for start, end in span_annotations:\n",
    "    chunk_start, chunk_end = None, None\n",
    "    for i, (start_offset, end_offset) in enumerate(offset_mapping):\n",
    "        if start_offset == start and end_offset < end:\n",
    "            chunk_start = i\n",
    "        if start_offset > start and end_offset == end:\n",
    "            chunk_end = i\n",
    "            break\n",
    "\n",
    "    chunk_positions.append((chunk_start, chunk_end))\n",
    "\n",
    "chunk_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d49fe854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "sep_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3ea9d7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flash attention implementation does not support kwargs: offset_mapping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 42, 1024]), torch.Size([1, 1024]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**inputs)\n",
    "\n",
    "output[0].shape, output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "368f7e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0067, -2.3910,  1.0657,  ..., -0.1316, -0.2900, -0.1048])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e75ef7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Berlin is the capital and largest city of Germany, both by area and by population.\n",
    "Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\n",
    "The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litserve-examples (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
